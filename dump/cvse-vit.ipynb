{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nimport tensorflow_datasets as tfds\nimport numpy as np\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\n\n# Custom Dataset to convert TensorFlow dataset to PyTorch\nclass PatchCamelyonDataset(Dataset):\n    def __init__(self, tfds_dataset, transform=None):\n        self.dataset = tfds_dataset\n        self.transform = transform\n        # Convert to list for indexing\n        self.data = list(tfds.as_numpy(self.dataset))\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        image = item['image'].astype(np.float32) / 255.0  # Normalize to [0,1]\n        label = item['label'].astype(np.float32)  # Float for BCEWithLogitsLoss\n        # Transpose image to PyTorch format (C, H, W)\n        image = np.transpose(image, (2, 0, 1))\n        image = torch.from_numpy(image)  # Convert to tensor\n        if self.transform:\n            image = self.transform(image)\n        return image, label\n\n# Debug data loading\ndef debug_dataset(dataset, num_samples=5):\n    for i in range(num_samples):\n        image, label = dataset[i]\n        print(f\"Sample {i}: Image shape: {image.shape}, Label: {label}, Image min: {image.min()}, Image max: {image.max()}\")\n\n# Define the Vision Transformer components\nclass PatchEmbedding(nn.Module):\n    def __init__(self, img_size, patch_size, in_channels, embed_dim):\n        super(PatchEmbedding, self).__init__()\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.n_patches = (img_size // patch_size) ** 2\n        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, x):\n        x = self.proj(x)  # (B, embed_dim, H/patch_size, W/patch_size)\n        x = x.flatten(2)  # (B, embed_dim, n_patches)\n        x = x.transpose(1, 2)  # (B, n_patches, embed_dim)\n        return x\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, n_patches, embed_dim):\n        super(PositionalEncoding, self).__init__()\n        self.pos_embed = nn.Parameter(torch.zeros(1, n_patches + 1, embed_dim))\n        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n\n    def forward(self, x):\n        B = x.shape[0]\n        cls_tokens = self.cls_token.expand(B, -1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)\n        x += self.pos_embed\n        return x\n\nclass TransformerEncoder(nn.Module):\n    def __init__(self, embed_dim, num_heads, ff_dim, num_layers, dropout):\n        super(TransformerEncoder, self).__init__()\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=embed_dim,\n            nhead=num_heads,\n            dim_feedforward=ff_dim,\n            dropout=dropout,\n            activation='gelu'\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n\n    def forward(self, x):\n        return self.transformer(x)\n\nclass VisionTransformer(nn.Module):\n    def __init__(self, img_size, patch_size, in_channels, embed_dim, num_heads, ff_dim, num_layers, dropout):\n        super(VisionTransformer, self).__init__()\n        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n        self.pos_embed = PositionalEncoding(self.patch_embed.n_patches, embed_dim)\n        self.transformer = TransformerEncoder(embed_dim, num_heads, ff_dim, num_layers, dropout)\n        self.mlp_head = nn.Sequential(\n            nn.LayerNorm(embed_dim),\n            nn.Linear(embed_dim, 1)  # Single output for BCEWithLogitsLoss\n        )\n\n    def forward(self, x):\n        x = self.patch_embed(x)\n        x = self.pos_embed(x)\n        x = self.transformer(x)\n        x = x[:, 0]  # Take the CLS token\n        x = self.mlp_head(x)\n        return x\n\n# Hyperparameters\nimg_size = 96\npatch_size = 16\nin_channels = 3\nembed_dim = 384  # Smaller for faster convergence\nnum_heads = 6\nff_dim = 1536\nnum_layers = 6  # Simpler model to avoid overfitting\ndropout = 0.1\nbatch_size = 64  # Conservative for Kaggle\nlearning_rate = 1e-4\nnum_epochs = 10\n\n# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Data preprocessing\ntransform = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomVerticalFlip(),\n    transforms.RandomRotation(90),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\n# Load Patch Camelyon dataset\ntrain_ds, train_info = tfds.load('patch_camelyon', split='train', as_supervised=False, with_info=True)\nval_ds = tfds.load('patch_camelyon', split='validation', as_supervised=False)\ntest_ds = tfds.load('patch_camelyon', split='test', as_supervised=False)\n\n# Wrap TensorFlow datasets in PyTorch Dataset\ntrain_dataset = PatchCamelyonDataset(train_ds, transform=transform)\nval_dataset = PatchCamelyonDataset(val_ds, transform=transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]))\ntest_dataset = PatchCamelyonDataset(test_ds, transform=transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]))\n\n# Debug dataset\nprint(\"Debugging train dataset:\")\ndebug_dataset(train_dataset)\n\n# Create DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=0)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=0)\n\n# Initialize the model\nmodel = VisionTransformer(\n    img_size=img_size,\n    patch_size=patch_size,\n    in_channels=in_channels,\n    embed_dim=embed_dim,\n    num_heads=num_heads,\n    ff_dim=ff_dim,\n    num_layers=num_layers,\n    dropout=dropout\n).to(device)\n\n# Loss and optimizer\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\nscheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n\n# Training loop\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss = 0.0\n    train_correct = 0\n    train_total = 0\n    for images, labels in train_loader:\n        images = images.to(device)\n        labels = labels.to(device).view(-1, 1)  # Reshape for BCEWithLogitsLoss\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n        predicted = (torch.sigmoid(outputs) > 0.5).float()\n        train_total += labels.size(0)\n        train_correct += (predicted == labels).sum().item()\n\n    train_accuracy = train_correct / train_total\n    scheduler.step()\n\n    # Validation\n    model.eval()\n    val_loss = 0.0\n    val_correct = 0\n    val_total = 0\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images = images.to(device)\n            labels = labels.to(device).view(-1, 1)\n            outputs = model(images)\n            val_loss += criterion(outputs, labels).item()\n            predicted = (torch.sigmoid(outputs) > 0.5).float()\n            val_total += labels.size(0)\n            val_correct += (predicted == labels).sum().item()\n\n    val_accuracy = val_correct / val_total\n    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss/len(train_loader):.4f}, '\n          f'Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss/len(val_loader):.4f}, '\n          f'Val Accuracy: {val_accuracy:.4f}')\n\n# Test the model\nmodel.eval()\ntest_loss = 0.0\ntest_correct = 0\ntest_total = 0\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images = images.to(device)\n        labels = labels.to(device).view(-1, 1)\n        outputs = model(images)\n        test_loss += criterion(outputs, labels).item()\n        predicted = (torch.sigmoid(outputs) > 0.5).float()\n        test_total += labels.size(0)\n        test_correct += (predicted == labels).sum().item()\n\ntest_accuracy = test_correct / test_total\nprint(f'Test Loss: {test_loss/len(test_loader):.4f}, Test Accuracy: {test_accuracy:.4f}')\ntorch.save(model.state_dict(), '/kaggle/working/my_model.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T19:37:21.320730Z","iopub.execute_input":"2025-04-18T19:37:21.320927Z","execution_failed":"2025-04-18T21:00:32.280Z"}},"outputs":[{"name":"stderr","text":"2025-04-18 19:37:33.230768: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745005053.452390      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745005053.515885      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Downloading and preparing dataset 7.48 GiB (download: 7.48 GiB, generated: Unknown size, total: 7.48 GiB) to /root/tensorflow_datasets/patch_camelyon/2.0.0...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Dl Completed...: 0 url [00:00, ? url/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ae8ac281257482798f59f35d1531683"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Dl Size...: 0 MiB [00:00, ? MiB/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"454a0a151cba48d59eb0236004272f5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extraction completed...: 0 file [00:00, ? file/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f67b55e7174b446a93e45be9929b8d5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test examples...:   0%|          | 0/32768 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"I0000 00:00:1745005249.218179      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Shuffling /root/tensorflow_datasets/patch_camelyon/incomplete.DDKWMA_2.0.0/patch_camelyon-test.tfrecord*...:  …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train examples...:   0%|          | 0/262144 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Shuffling /root/tensorflow_datasets/patch_camelyon/incomplete.DDKWMA_2.0.0/patch_camelyon-train.tfrecord*...: …","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation examples...:   0%|          | 0/32768 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Shuffling /root/tensorflow_datasets/patch_camelyon/incomplete.DDKWMA_2.0.0/patch_camelyon-validation.tfrecord*…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset patch_camelyon downloaded and prepared to /root/tensorflow_datasets/patch_camelyon/2.0.0. Subsequent calls will reuse this data.\nDebugging train dataset:\nSample 0: Image shape: torch.Size([3, 96, 96]), Label: 0.0, Image min: -1.0, Image max: 1.0\nSample 1: Image shape: torch.Size([3, 96, 96]), Label: 0.0, Image min: -1.0, Image max: 1.0\nSample 2: Image shape: torch.Size([3, 96, 96]), Label: 1.0, Image min: -1.0, Image max: 1.0\nSample 3: Image shape: torch.Size([3, 96, 96]), Label: 1.0, Image min: -1.0, Image max: 1.0\nSample 4: Image shape: torch.Size([3, 96, 96]), Label: 1.0, Image min: -1.0, Image max: 1.0\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Train Loss: 0.6956, Train Accuracy: 0.5004, Val Loss: 0.6939, Val Accuracy: 0.4995\nEpoch 2/10, Train Loss: 0.6936, Train Accuracy: 0.5018, Val Loss: 0.6932, Val Accuracy: 0.5005\nEpoch 3/10, Train Loss: 0.6935, Train Accuracy: 0.5008, Val Loss: 0.6935, Val Accuracy: 0.5005\nEpoch 4/10, Train Loss: 0.6934, Train Accuracy: 0.5003, Val Loss: 0.6933, Val Accuracy: 0.4995\nEpoch 5/10, Train Loss: 0.6933, Train Accuracy: 0.4993, Val Loss: 0.6932, Val Accuracy: 0.5005\nEpoch 6/10, Train Loss: 0.6932, Train Accuracy: 0.4990, Val Loss: 0.6931, Val Accuracy: 0.4995\nEpoch 7/10, Train Loss: 0.6932, Train Accuracy: 0.4990, Val Loss: 0.6932, Val Accuracy: 0.4995\nEpoch 8/10, Train Loss: 0.6932, Train Accuracy: 0.5002, Val Loss: 0.6932, Val Accuracy: 0.5005\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}